{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93e3d229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pprint\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "import matplotlib\n",
    "from os import listdir\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import sys\n",
    "import pickle\n",
    "import miceforest as mf\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import functools\n",
    "from functools import reduce\n",
    "from mc4.algorithm import mc4_aggregator\n",
    "from copy import deepcopy\n",
    "\n",
    "import sklearn.neighbors._base\n",
    "sys.modules['sklearn.neighbors.base'] = sklearn.neighbors._base\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import missingpy\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import brier_score_loss, precision_score, recall_score, f1_score, log_loss, roc_auc_score\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from collections import Counter\n",
    "\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7803c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8ba2520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def _init_fn(worker_id):\n",
    "    np.random.seed(int(rs))\n",
    "    \n",
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_torch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59e5baac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersample Test Set\n",
    "test_under_sampler = 0\n",
    "do_class_weight = 1\n",
    "\n",
    "# Undersample Train Set\n",
    "do_smote = 0\n",
    "do_random_under_sampler = 0\n",
    "do_random_over_sampler = 0\n",
    "do_tomek_links = 0\n",
    "do_near_miss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15fdd315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_csv_filenames( path_to_dir, suffix=\".csv\" ):\n",
    "    filenames = listdir(path_to_dir)\n",
    "    return [ filename for filename in filenames if filename.endswith( suffix ) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd826a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "539d6fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_window = \"48h\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43e5c5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = 'AKI Data/eicu_aki/new data/'\n",
    "prediction_directory = 'observation' + prediction_window + '_prediction24h/'\n",
    "\n",
    "data_files = find_csv_filenames(data_directory+prediction_directory)\n",
    "\n",
    "hosps = [x.replace('.csv', '') for x in data_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82759b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = 'AKI Models/' + prediction_window + '/Pytorch LR/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ddcfee",
   "metadata": {},
   "source": [
    "#### Loading Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17c5edff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_outliers(data):\n",
    "    demographic_index = list(data).index('is_female')\n",
    "    medical_signs = list(data)[:demographic_index]\n",
    "    demographic_data = ['is_female', 'age', 'race_black', 'race_hispanic', 'race_asian', \n",
    "                        'race_other', 'electivesurgery', 'BMI']\n",
    "    demographic_data = list(set(list(data)) & set(demographic_data))\n",
    "    numerical_col = medical_signs + list(set(list(data)) & set(['age', 'BMI']))\n",
    "    for col in numerical_col:\n",
    "        min_value = data[col].quantile(0.01)\n",
    "        max_value = data[col].quantile(0.99)\n",
    "        data[col][data[col] < min_value] = None\n",
    "        data[col][data[col] > max_value] = None\n",
    "    return data\n",
    "\n",
    "def get_features_diag(data):\n",
    "    features = list(data_dictionary[hosp])\n",
    "    features.remove(list(data_dictionary[hosp])[-1])\n",
    "    AKI_diag = list(data_dictionary[hosp])[-1]\n",
    "    return features, AKI_diag\n",
    "\n",
    "def do_training_modifications(X_train, y_train):\n",
    "    if do_smote:\n",
    "        oversample = SMOTE(random_state=rs)\n",
    "        X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "    if do_random_under_sampler:\n",
    "        rus = RandomUnderSampler(replacement=True, random_state=rs)\n",
    "        X_train, y_train = rus.fit_resample(X_train, y_train)\n",
    "    if do_random_over_sampler:\n",
    "        ros = RandomOverSampler(random_state=rs)\n",
    "        X_train, y_train = ros.fit_resample(X_train, y_train)\n",
    "    if do_tomek_links:\n",
    "        tl = TomekLinks(random_state=rs)\n",
    "        X_train, y_train = tl.fit_resample(X_train, y_train)\n",
    "    if do_near_miss:\n",
    "        nm = NearMiss(random_state=rs)\n",
    "        X_train, y_train = nm.fit_resample(X_train, y_train)  \n",
    "    return X_train, y_train\n",
    "\n",
    "def normalize_data(data_dictionary, hosp):\n",
    "    df = deepcopy(data_dictionary[hosp])\n",
    "    demographic_index = list(df).index('is_female')\n",
    "    medical_signs = list(df)[:demographic_index]\n",
    "    medication_index = list(df).index('ACETAMIN')\n",
    "    medications = list(df)[medication_index:len(list(data))-1]\n",
    "    demographic_binary = ['is_female', 'race_black', 'race_hispanic', 'race_asian', \n",
    "                        'race_other', 'electivesurgery']\n",
    "    binary_features = medications + demographic_binary\n",
    "    continuous_features = medical_signs + ['age', 'BMI']\n",
    "\n",
    "    df[continuous_features] = (df[continuous_features]-df[continuous_features].mean())/df[continuous_features].std()\n",
    "    df[binary_features] = df[binary_features].replace([0],-1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f186800",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data_dictionary = dict.fromkeys(hosps)\n",
    "missing_dictionary = dict.fromkeys(hosps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "381b066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = find_csv_filenames(data_directory+prediction_directory)\n",
    "missing_features = []\n",
    "\n",
    "for file in data_files:\n",
    "    data = pd.read_csv(data_directory + prediction_directory + file)\n",
    "    hosp = file.replace('.csv', '')\n",
    "\n",
    "    data = clean_outliers(data)\n",
    "\n",
    "    df_missing = data.isnull().sum()\n",
    "    all_cols = list(df_missing.index)\n",
    "    missing_vals = list(df_missing)\n",
    "\n",
    "    indices_remove = []\n",
    "    for i in range(0, len(missing_vals)):\n",
    "        if missing_vals[i] == 0:\n",
    "            indices_remove.append(i)\n",
    "\n",
    "    big_array = np.array([all_cols,missing_vals])\n",
    "    missing_feats, num_missing = np.delete(big_array, indices_remove, axis=1)\n",
    "    df_missing = pd.DataFrame(data={\"Feature\":missing_feats, 'Number Missing':num_missing})\n",
    "\n",
    "    missing_dictionary[hosp] = df_missing\n",
    "    df_missing[\"Frequency Missing\"] = (df_missing[\"Number Missing\"].values).astype(int) / data.shape[0]\n",
    "    large_missing_features = df_missing[df_missing[\"Frequency Missing\"] > 0.999]\n",
    "    missing_features.append(large_missing_features[\"Feature\"].values)\n",
    "\n",
    "    temp_data_dictionary[hosp] = data\n",
    "\n",
    "missing_features = list(np.unique(np.concatenate(missing_features)))\n",
    "\n",
    "for num, hosp in enumerate(temp_data_dictionary):\n",
    "    data = temp_data_dictionary[hosp]\n",
    "    data = data.drop(list(missing_features), axis=1)\n",
    "    temp_data_dictionary[hosp] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdc8e926",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dictionary = dict.fromkeys(hosps)\n",
    "\n",
    "for hosp in data_dictionary:\n",
    "    file = str(hosp) + '.csv'\n",
    "    data = pd.read_csv(data_directory + prediction_directory + \"cleaned/\" + file)\n",
    "    data = data.loc[:, ~data.columns.str.contains('^Unnamed')]\n",
    "    data_dictionary[hosp] = data\n",
    "\n",
    "df = deepcopy(data_dictionary[hosp])\n",
    "demographic_index = list(df).index('is_female')\n",
    "medical_signs = list(df)[:demographic_index]\n",
    "medication_index = list(df).index('ACETAMIN')\n",
    "medications = list(df)[medication_index:len(list(data))-1]\n",
    "demographic_binary = ['is_female', 'race_black', 'race_hispanic', 'race_asian', \n",
    "                    'race_other', 'electivesurgery']\n",
    "binary_features = medications + demographic_binary\n",
    "continuous_features = medical_signs + ['age', 'BMI']\n",
    "\n",
    "for hosp in hosps:\n",
    "    orig_data = temp_data_dictionary[hosp]\n",
    "    data = data_dictionary[hosp]\n",
    "    \n",
    "    for col in continuous_features:\n",
    "        maximum = orig_data[col].max(axis=0)\n",
    "        minimum = orig_data[col].min(axis=0)\n",
    "        data[col] = data[col] * (maximum - minimum) + minimum\n",
    "        \n",
    "    data_dictionary[hosp] = data\n",
    "    \n",
    "    file = str(hosp) + '.csv'\n",
    "    data.to_csv(data_directory + prediction_directory + \"cleaned/unscaled/\" + file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
